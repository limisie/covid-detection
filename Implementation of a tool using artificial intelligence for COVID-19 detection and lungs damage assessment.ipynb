{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3250a027",
   "metadata": {},
   "source": [
    "# Implementation of a tool using artificial intelligence to analyse chest X-rays for COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 101\n",
    "BATCH_SIZE = 8\n",
    "CLASSES = 2\n",
    "\n",
    "DATA_ROOT = './data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7895a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9fb515",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "### Removing duplicates\n",
    "Duplicates are removed using a perceptual hashing algorithm. The goal is to reveal number of unique images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import imagehash\n",
    "\n",
    "DATA_ROOT = './data/original/'\n",
    "\n",
    "MAX_DIFFERENCE = 8\n",
    "hashes = []\n",
    "duplicate_groups = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43113333",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob(f'{DATA_ROOT}*')\n",
    "images = sorted(images)\n",
    "\n",
    "for i, img_path in enumerate(images):\n",
    "    img = Image.open(img_path)\n",
    "    img_hash = imagehash.phash(img)\n",
    "    hashes.append(img_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc16a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, hsh in enumerate(hashes):\n",
    "    hsh_duplicates = tuple([img_path for h, img_path in zip(hashes, images) if hsh - h < MAX_DIFFERENCE])\n",
    "    if len(hsh_duplicates) > 1:\n",
    "        duplicate_groups.add(hsh_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c5492",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "Images in train set are randomly rotated, flipped and their brightness is modified, to increase the number of COVID-19 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Augmentor\n",
    "\n",
    "DATA_TO_AUGMENT = './data/original/train/covid'\n",
    "\n",
    "augmented_samples_count = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Augmentor.Pipeline(DATA_TO_AUGMENT)\n",
    "\n",
    "p.rotate(probability=0.8, max_left_rotation=15, max_right_rotation=15)\n",
    "p.flip_left_right(probability=0.2)\n",
    "p.random_brightness(0.6, 0.5, 1.2)\n",
    "p.set_save_format(save_format=\"PNG\")\n",
    "\n",
    "p.sample(augmented_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dafba0a",
   "metadata": {},
   "source": [
    "### Lung segmentation\n",
    "Before feeding data to models, CXR images are segmented using trained VAE. The code is available in another notebook that can be found in utils direcotry.\n",
    "### CLAHE\n",
    "To get rid of differences in contrast and brightness in the analyzed images, CLAHE is applied to the images. After the modification, segmented lung masks are overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "MASKS_DIR = './data/masks/'\n",
    "OUTPUT_DIR = './data/segmented/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = glob.glob(f'{MASKS_DIR}*')\n",
    "masks = sorted(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img_path, mask_path in enumerate(zip(images, masks)):\n",
    "    output_path = f'{OUTPUT_DIR}{img_path.split('/')[-1].split('.')[0]}_preprocessed.png'\n",
    "    img = cv2.imread(img_path)\n",
    "    mask = cv2.imread(mask_path)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cl1 = clahe.apply(img)\n",
    "    \n",
    "    result = cv2.bitwise_and(cl1, mask)\n",
    "    cv2.imwrite(output_path, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9867c7",
   "metadata": {},
   "source": [
    "### Normalization and standarization\n",
    "Data is preprocessed with torchvision.transforms. Images in train set is randomly resized and cropped. All images are resized to the required size (224x224x3) and normalized. The process of computing the mean and standard deviation of the dataset can be found in utils directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.1220, 0.1220, 0.1220], [0.2058, 0.2058, 0.2058])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.1220, 0.1220, 0.1220], [0.2058, 0.2058, 0.2058])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.1220, 0.1220, 0.1220], [0.2058, 0.2058, 0.2058])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b2a1e",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Data is loaded into data set with ImageFolder from root directory that contains train, validation and data. All folders consist of class directories (covid and non-covid). \n",
    "\n",
    "Then the data is passed to DataLoader that shuffles data and provides samples in minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_ROOT, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "data_loaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True, \n",
    "                                               num_workers=4)\n",
    "               for x in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a107b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZES = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "CLASS_NAMES = image_datasets['train'].classes\n",
    "print(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a55a2",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "Functions to handle different types of image format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853eeace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_img(arr):\n",
    "    arr = arr.transpose((1, 2, 0))\n",
    "    mean = np.array([0.1220, 0.1220, 0.1220])\n",
    "    std = np.array([0.2058, 0.2058, 0.2058])\n",
    "    arr = std * arr + mean\n",
    "    img = np.clip(arr, 0, 1)\n",
    "    img = (arr * 255).astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a34e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_img(tensor):\n",
    "    arr = tensor.numpy()\n",
    "    return numpy_to_img(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show(inp, title=None):\n",
    "    if isinstance(inp, torch.Tensor):\n",
    "        img = tensor_to_img(inp)\n",
    "    elif isinstance(inp, np.ndarray):\n",
    "        img = numpy_to_img(inp)\n",
    "    else:\n",
    "        img = inp\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b82440",
   "metadata": {},
   "source": [
    "Function to visualize data batch that is in tensor format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_batch(inp, title=None):\n",
    "    inp = tensor_to_img(inp)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "NUMBER_OF_SAMPLES = 4\n",
    "\n",
    "inputs, labels = next(iter(data_loaders['train']))\n",
    "out = utils.make_grid(inputs[:NUMBER_OF_SAMPLES])\n",
    "display_batch(out, title=[CLASS_NAMES[x] for x in labels[:NUMBER_OF_SAMPLES]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d6fdd",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "There are two major transfer learning approaches. First one consists of initializing the network with pretrained weights instead of random initialization, and training it as usual. Second approach adapts fixed pretrained network as a feature extractor, where only the last fully connected layer is randomly initialized and trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e6f1c",
   "metadata": {},
   "source": [
    "## ResNet18 as feature extractor\n",
    "Pretrained model of ResNet18 is loaded. To exploit it as feature extractor, parameters must be fixed. The requires_grad flag deactivates autograd engine and freezes the parameters so the memory is saved and trainig speeds up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db01b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_resnet18 = models.resnet18(pretrained=True)\n",
    "for param in m_resnet18.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d994ff",
   "metadata": {},
   "source": [
    "### ResNet18 architecure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e635852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(m_resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9560cf3",
   "metadata": {},
   "source": [
    "Next, the last fully connected layer is replaced with linear fully connected layer with 2 outputs for two-class classification. Then the model is allocated on the device (depending on CUDA availability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903baa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = m_resnet18.fc.in_features\n",
    "m_resnet18.fc = torch.nn.Linear(in_features=num_ftrs, out_features=CLASSES)\n",
    "\n",
    "m_resnet18 = m_resnet18.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ccde36",
   "metadata": {},
   "source": [
    "Additional Softmax layer might be applied to get the class probabilites at output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257deee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = m_resnet18.fc.in_features\n",
    "m_resnet18.fc = nn.Sequential(\n",
    "    nn.Linear(in_features=num_ftrs, out_features=CLASSES),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "m_resnet18 = m_resnet18.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78988e6",
   "metadata": {},
   "source": [
    "Then total number of parameters and trainable parameters can be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in m_resnet18.parameters())\n",
    "pytorch_trainable_params = sum(p.numel() for p in m_resnet18.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Total parameters: {pytorch_total_params}')\n",
    "print(f'Trainable parameters: {pytorch_trainable_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4454f32",
   "metadata": {},
   "source": [
    "## Other models\n",
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "for param in m_resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_ftrs = m_resnet50.fc.in_features\n",
    "m_resnet50.fc = nn.Sequential(\n",
    "    nn.Linear(in_features=num_ftrs, out_features=CLASSES),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "m_resnet50 = m_resnet50.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d804f6",
   "metadata": {},
   "source": [
    "### SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_squeezenet = models.squeezenet1_1(pretrained=True)\n",
    "\n",
    "for param in m_squeezenet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "m_squeezenet.classifier._modules[\"1\"] = nn.Sequential(\n",
    "    nn.Conv2d(512, CLASSES, kernel_size=(1, 1)),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "m_squeezenet.num_classes = CLASSES\n",
    "\n",
    "m_squeezenet = m_squeezenet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243589a",
   "metadata": {},
   "source": [
    "### DenseNet-121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_densenet = models.densenet121(pretrained=True)\n",
    "\n",
    "for param in m_densenet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = m_densenet.classifier.in_features\n",
    "m_densenet.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=num_ftrs, out_features=CLASSES),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "m_densenet = m_densenet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978b38c",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "## Learning rate finder\n",
    "To find a range of values of learning rate that enable model to converge, the Leslie Smith's method is eployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "model = m_resnet18\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-7)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "lr_finder.range_test(data_loaders['train'], val_loader=data_loaders['val'], end_lr=10, num_iter=100)\n",
    "lr_finder.plot(suggest_lr=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4ba8f",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Loss function\n",
    "Chosen criterion is cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865912f9",
   "metadata": {},
   "source": [
    "Function to train and validate model and save the best parameters. Each epoch has a training and a validation phase. If the epoch accuracy is better than best accuracy, then the model is saved. The function returns the best model that came along during the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a674fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler=None, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    losses = {'train': [],\n",
    "              'val': []}\n",
    "    accs = {'train': [],\n",
    "              'val': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in data_loaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train' and scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / DATASET_SIZES[phase]\n",
    "            epoch_acc = running_corrects.double() / DATASET_SIZES[phase]\n",
    "            \n",
    "            losses[phase].append(epoch_loss)\n",
    "            accs[phase].append(epoch_acc.item())\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss} Acc: {epoch_acc}')\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60}m {time_elapsed % 60}s')\n",
    "    print(f'Best val Acc: {best_acc}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, losses, accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f595cd2",
   "metadata": {},
   "source": [
    "Chosen model is trained for selected number of epochs. The implementation supports early stopping technique - model with the best validation accuracy is saved during the training and then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = m_resnet18\n",
    "label = \"ResNet18\"\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853afa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, loss, acc= train_model(model,\n",
    "                              criterion, \n",
    "                              optimizer,\n",
    "                              scheduler,\n",
    "                              num_epochs=EPOCHS)\n",
    "\n",
    "torch.save(model, f'covid_{label}_epochs{EPOCHS}_{optimizer.__class__.__name__}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccedf7f",
   "metadata": {},
   "source": [
    "The training and validation loss and accuracy values are saved for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rn18_loss', 'wb') as f:\n",
    "    pickle.dump(loss, f)\n",
    "    \n",
    "with open('rn18_acc', 'wb') as f:\n",
    "    pickle.dump(loss, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b6b1e",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Function that returns predictions and probabilities of predicted classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb88822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "#     sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    class_probabilities = {0: [],\n",
    "                           1: []}\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "#             out_probs = sm(outputs)\n",
    "\n",
    "            for prob, label in zip(out_probs, labels):\n",
    "                label = label.item()\n",
    "                probability = prob[label].item()\n",
    "                \n",
    "                ground_truths.append(label)\n",
    "                class_probabilities[label].append(probability)\n",
    "                probabilities.append(prob[0].item())\n",
    "                predictions.append(prob.max(0, keepdim=True).indices.item())\n",
    "        \n",
    "    return np.asarray(ground_truths), np.asarray(predictions), np.array(probabilities), np.asarray(class_probabilities[0]), np.asarray(class_probabilities[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b090d2",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15388a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score, PrecisionRecallDisplay\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt, cycler, ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_m(labels, predictions):\n",
    "    confusion_matrix = np.zeros((2, 2))\n",
    "\n",
    "    for lbl, pred in zip(labels, predictions):\n",
    "        confusion_matrix[lbl, pred] += 1\n",
    "\n",
    "    return confusion_matrix \n",
    "\n",
    "def plot_cf(cf):\n",
    "    df_cm = pd.DataFrame(cf, \n",
    "                         index = [i for i in CLASS_NAMES],\n",
    "                         columns = [i for i in CLASS_NAMES])\n",
    "\n",
    "    ax = sn.heatmap(df_cm, annot=True, fmt='g')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.xlabel('predicted')\n",
    "    plt.ylabel('labels')\n",
    "    plt.figure(figsize = (7,5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39626de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_sen_spe(confusion_matrix):\n",
    "    TP = confusion_matrix[0, 0]\n",
    "    FN = confusion_matrix[0, 1]\n",
    "    FP = confusion_matrix[1, 0]\n",
    "    TN = confusion_matrix[1, 1]\n",
    "    \n",
    "    accuracy = (TP + TN) / np.sum(confusion_matrix)\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (FP + TN)\n",
    "    \n",
    "    return accuracy, sensitivity, specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0358d5f",
   "metadata": {},
   "source": [
    "### Plot loss and accuracy graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc9cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_SIZE_X = 10\n",
    "PLOT_SIZE_Y = 5\n",
    "PLOT_LEFT_POS = 0.1\n",
    "PLOT_RIGHT_POS = 0.9\n",
    "PLOT_BOTTOM_POS = 0.15\n",
    "PLOT_TOP_POS = 0.85\n",
    "PLOT_MARGIN = 0.01\n",
    "PLOT_LW = 0.9\n",
    "PLOT_GRID_LW = 0.2\n",
    "PLOT_TICKS_Y_INTERVAL = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad12086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(arr, title='loss'):\n",
    "    \n",
    "    colors = cycler('color', ['orange', 'dodgerblue'])\n",
    "    plt.rc('axes', prop_cycle=colors)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_SIZE_X, PLOT_SIZE_Y))\n",
    "    plt.subplots_adjust(left=PLOT_LEFT_POS, right=PLOT_RIGHT_POS, bottom=PLOT_BOTTOM_POS, top=PLOT_TOP_POS)\n",
    "    plt.margins(x=PLOT_MARGIN)\n",
    "    \n",
    "    plt.plot(range(0, len(arr['train'])), arr['train'], label='Training', linewidth=PLOT_LW)\n",
    "    plt.plot(range(0, len(arr['val'])), arr['val'], label='Validation', linewidth=PLOT_LW)\n",
    "\n",
    "    plt.title(f\"Training and validation {title}\")\n",
    " \n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(title)\n",
    "\n",
    "    legend = plt.legend(loc='upper right')\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor('white')\n",
    "\n",
    "    plt.grid(axis='y', lw=PLOT_GRID_LW)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator((len(arr['train'])) / 5))\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator((len(arr['train'])) / 50))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(PLOT_TICKS_Y_INTERVAL))\n",
    "    ax.yaxis.set_minor_locator(ticker.MultipleLocator(PLOT_TICKS_Y_INTERVAL / 5))\n",
    "    ax.set_xlim(xmin=0)\n",
    "    ax.set_xlim(xmax=len(arr['train']) - 1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93a062",
   "metadata": {},
   "source": [
    "### Plot prediction histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(data, labels, bins=10, model_name=\"\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.hist(data, label=labels, bins=bins, range=(0,1))\n",
    "    \n",
    "    plt.title(f\"Predicted probabilities with {model_name}\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(axis='y', lw=PLOT_GRID_LW)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "    plt.xlabel(\"probability\")\n",
    "    plt.ylabel(\"counts\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d92fe8",
   "metadata": {},
   "source": [
    "### Plot ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c912f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(true_labels, preds, labels):\n",
    "    TICKER = 0.1\n",
    "    _, ax = plt.subplots(figsize=(7,6))\n",
    "\n",
    "    for tl, p, lbl in zip(true_labels, preds, labels):\n",
    "        auc = roc_auc_score(1 - tl, p)\n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(1 - tl, p)\n",
    "\n",
    "        plt.plot(false_positive_rate, \n",
    "                 true_positive_rate, \n",
    "                 label=f'{lbl},     AUC={auc:.4f}')\n",
    "        \n",
    "        gmeans = np.sqrt(true_positive_rate * (1 - false_positive_rate))\n",
    "        ix = np.argmax(gmeans)\n",
    "        print(f'Best Threshold={thresholds[ix]}, G-Mean={gmeans[ix]}')\n",
    "    \n",
    "    x = np.linspace(0, 1)\n",
    "    plt.plot(x, x, linestyle='--', label='Baseline', color='silver')\n",
    "    \n",
    "    plt.ylim([0, 1.05])\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(\"ROC Curve\")\n",
    "    \n",
    "    legend = plt.legend(loc='lower right')\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor('white')\n",
    "    plt.grid(axis='y', lw=0.2)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(TICKER))\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(TICKER / 5))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(TICKER))\n",
    "    ax.yaxis.set_minor_locator(ticker.MultipleLocator(TICKER / 5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037fc52",
   "metadata": {},
   "source": [
    "### Plot precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pre_rec(true_labels, preds, labels):\n",
    "    _, ax = plt.subplots(figsize=(7,6))\n",
    "        \n",
    "    for tl, p, lbl in zip(true_labels, preds, labels):\n",
    "        precision, recall, _ = precision_recall_curve(1 - tl, p)\n",
    "        average_precision = average_precision_score(1 - tl, p)\n",
    "        display = PrecisionRecallDisplay(recall=recall, precision=precision)\n",
    "        display.plot(ax=ax, name=f'{lbl},     AP={average_precision:.4f}')\n",
    "        baseline = np.sum(1 - tl) / len(tl)\n",
    "\n",
    "    ax.plot([0, 1], [baseline, baseline], linestyle='--', label='Baseline', color='silver')\n",
    "    \n",
    "    plt.ylim([0, 1.05])\n",
    "    plt.xlim([0, 1.05])\n",
    "    plt.title(\"Precision-Recall curve\")\n",
    "    \n",
    "    legend = plt.legend(loc='lower left')\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor('white')\n",
    "    plt.grid(axis='y', lw=0.2)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sens_spec(probabilities, labels, threshold):\n",
    "    predictions = np.where(probabilities >= threshold, 0, 1)\n",
    "\n",
    "    confusion_matrix = cf_m(labels, predictions)\n",
    "    accuracy, sensitivity, specificity = get_acc_sen_spe(confusion_matrix)\n",
    "    \n",
    "    return accuracy, sensitivity, specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff4190",
   "metadata": {},
   "source": [
    "# Evaluation code\n",
    "Prepare data and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0310aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_ROOT = './data/segmented/test'\n",
    "\n",
    "test_dataset = datasets.ImageFolder(TEST_DATA_ROOT, data_transforms['test'])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "test_size = len(test_dataset)\n",
    "CLASS_NAMES = test_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7dcdfb",
   "metadata": {},
   "source": [
    "Load models from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './models/ResNet18_Adam_0.0001_100E.pt'\n",
    "LABEL = 'ResNet18'\n",
    "\n",
    "model = torch.load(MODEL_PATH, map_location=device)\n",
    "target_layer = [model.layer4[-1]]\n",
    "\n",
    "# target_layer = [model.features[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9897f",
   "metadata": {},
   "source": [
    "Make predictions and get class probabilites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec6ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels, predictions, probabilities, covid_probs, non_probs = get_predictions(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d9018",
   "metadata": {},
   "source": [
    "Plot confusion matrix and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7594619",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = cf_m(labels, predictions)\n",
    "plot_cf(confusion_matrix)\n",
    "accuracy, sensitivity, specificity = get_acc_sen_spe(confusion_matrix)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Sensitivity: {sensitivity:.4f}')\n",
    "print(f'Specificity: {specificity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c585ca",
   "metadata": {},
   "source": [
    "Plot histogram, ROC curve and precision-recall curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram([covid_probs, non_probs], ['probabilities of COVID cases', 'probabilities of non-COVID cases'])\n",
    "plot_roc([labels], [probabilities], [LABEL])\n",
    "plot_pre_rec([labels], [probabilities], [LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.1, 0.15, 0.24, 0.3, 0.4, 0.5]\n",
    "\n",
    "print('Threshold\\tSensitivity\\tSpecificity\\tAccuracy')\n",
    "for t in thresholds:\n",
    "    accuracy, sensitivity, specificity = find_sens_spec(probabilities, labels, t)\n",
    "    print(f'{t}\\t\\t{sensitivity:.4f}\\t\\t{specificity:.4f}\\t\\t{accuracy:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885a8af",
   "metadata": {},
   "source": [
    "Load loss and accuracy training and validation values and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rn18_loss', 'rb') as loss_file:\n",
    "    losses = pickle.load(loss_file)\n",
    "    \n",
    "with open('rn18_acc', 'rb') as acc_file:\n",
    "    accs = pickle.load(acc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(loss)\n",
    "plot(acc, title=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3f7d1",
   "metadata": {},
   "source": [
    "## GradCAM\n",
    "Enable gradient engine for all model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3536765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db381b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(img, model, target_layer, original=None):\n",
    "    \n",
    "    cam = GradCAM(model=model, target_layers=target_layer, use_cuda=False)\n",
    "    \n",
    "    target_category = None\n",
    "    grayscale_cam = cam(input_tensor=img, target_category=target_category, eigen_smooth=True)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    if original is not None:\n",
    "        rgb_img = np.float32(original) / 255\n",
    "    else:\n",
    "        rgb_img = np.float32(tensor_to_img(img[0])) / 255\n",
    "        \n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n",
    "    \n",
    "    return cv2.cvtColor(cam_image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a64e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmap(heatmap):\n",
    "    plt.imshow(heatmap)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353904a",
   "metadata": {},
   "source": [
    "Load a batch of data and plot the image with ground truth label and model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "\n",
    "imgs = imgs.to(device)\n",
    "labels = labels.to(device)\n",
    "img = imgs[0]\n",
    "label = CLASS_NAMES[labels[0]]\n",
    "\n",
    "pred = model(imgs)\n",
    "pred_label = CLASS_NAMES[pred[0].argmax(dim=0).item()]\n",
    "\n",
    "img_show(img.cpu(), f'L: {label} | P: {pred_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cff093",
   "metadata": {},
   "source": [
    "Generate a heatmap with GradCAM and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = grad_cam(imgs.cpu(), model.cpu(), target_layer)\n",
    "show_heatmap(heatmap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
